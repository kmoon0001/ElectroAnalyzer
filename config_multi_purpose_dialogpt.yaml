# Ultra-Light Clinical Document Analyzer with DialoGPT Medium Multi-Purpose
# Strategic use of DialoGPT Medium across multiple functions for maximum efficiency

# Application Settings
app_name: "Therapy Compliance Analyzer"
version: "2.0.0"
debug: false

# AI Configuration
use_ai_mocks: false  # REAL AI with optimized model usage

# Database Configuration
database:
  url: "sqlite:///./compliance.db"
  echo: false

# LLM Configuration - Meditron 7B Q4_K_S (Primary Medical Analysis)
llm:
  context_length: 4096
  gpu_layers: 0
  threads: 8
  hf_model_type: meditron
  model_type: ctransformers
  model_repo_id: TheBloke/meditron-7B-GGUF
  local_model_path: "models/meditron7b-ultra/meditron-7b.Q4_K_S.gguf"
  generation_params:
    max_new_tokens: 1024
    repeat_penalty: 1.1
    stop_sequences:
    - </analysis>
    - "\n\n---"
    temperature: 0.1
    top_p: 0.9

# Multi-Purpose DialoGPT Medium Configuration
dialogpt_medium:
  enabled: true
  local_path: "models/dialogpt-medium"
  repo: "microsoft/DialoGPT-medium"
  context_length: 1024
  max_tokens: 512
  temperature: 0.7

  # Multi-purpose usage
  functions:
    chat_assistant: true
    text_generation: true
    summary_generation: true
    report_narratives: true
    user_interface_text: true
    quick_responses: true

  # Performance settings
  batch_processing: true
  response_caching: true
  parallel_processing: true

# Chat Configuration - DialoGPT Medium
chat:
  enabled: true
  model: "dialogpt-medium"
  local_path: "models/dialogpt-medium"
  max_tokens: 256
  temperature: 0.7
  context_length: 512
  response_timeout: 5
  fact_check_responses: true
  medical_accuracy_mode: true

# Generator Profiles - Optimized Multi-Purpose
generator_profiles:
  clinical_primary:
    name: "Meditron 7B Q4_K_S - Primary Medical Analysis"
    description: "Main medical compliance analysis"
    generator: "meditron-7b-ultra"
    local_path: "models/meditron7b-ultra"
    repo: "TheBloke/meditron-7B-GGUF"
    model_file: "meditron-7b.Q4_K_S.gguf"
    context_length: 4096
    max_tokens: 1024
    temperature: 0.1
    quantization: "Q4_K_S"
    use_case: "primary_medical_analysis"

  dialogpt_multi_purpose:
    name: "DialoGPT Medium - Multi-Purpose Generator"
    description: "Text generation, summaries, narratives, chat"
    generator: "dialogpt-medium"
    local_path: "models/dialogpt-medium"
    repo: "microsoft/DialoGPT-medium"
    context_length: 1024
    max_tokens: 512
    temperature: 0.7
    type: "multi_purpose"
    functions: ["chat", "text_gen", "summary", "narrative"]
    performance_optimized: true

# Text Generation Strategy
text_generation:
  strategy: "hybrid"

  # Use Meditron for critical medical analysis
  medical_analysis:
    model: "meditron-7b-ultra"
    use_cases: ["compliance_scoring", "medical_reasoning", "clinical_findings"]

  # Use DialoGPT Medium for non-critical text generation
  general_text:
    model: "dialogpt-medium"
    use_cases: ["summaries", "narratives", "user_interface", "quick_responses"]

  # Performance settings
  parallel_generation: true
  batch_processing: true
  response_caching: true

# NER Models - Single Ultra-Light Model
ner_models:
  biomedical_ultra:
    name: "Ultra-Distilled Biomedical NER"
    model_id: "d4data/biomedical-ner-all"
    local_path: "models/biomedical-ner-ultra"
    priority: 1
    type: "ultra-distilled"
    size_reduction: "70%"
    fact_checking_enabled: true

# Embedding Models - Ultra-Light Version
embedding_models:
  distilbert_ultra:
    name: "DistilBERT Ultra-Light Embeddings"
    model_id: "sentence-transformers/distilbert-base-nli-mean-tokens"
    local_path: "models/distilbert-ultra"
    dimension: 768
    type: "ultra-distilled"
    size_reduction: "60%"

# Analysis Settings - Optimized Multi-Model Strategy
analysis:
  max_file_size_mb: 50
  supported_formats: ["pdf", "docx", "txt", "html"]
  phi_redaction: true
  confidence_threshold: 0.7
  fact_checking_enabled: true
  hallucination_detection: true
  medical_accuracy_mode: true

  # Model selection strategy
  model_selection:
    primary_medical: "meditron-7b-ultra"
    text_generation: "dialogpt-medium"
    chat_assistant: "dialogpt-medium"
    summary_generation: "dialogpt-medium"
    report_narratives: "dialogpt-medium"

# API Settings
api:
  host: "127.0.0.1"
  port: 8001
  cors_origins: ["http://127.0.0.1:3001", "http://localhost:3001"]

# Logging
logging:
  level: "INFO"
  format: "json"
  file: "logs/app.log"

# Cache Settings - Optimized for Multi-Model
cache:
  enabled: true
  ttl_seconds: 7200
  max_size_mb: 500
  fact_check_cache: true
  dialogpt_response_cache: true
  meditron_analysis_cache: true

# Ultra-Light Performance Settings with Multi-Purpose
ultra_light_mode:
  enabled: true
  single_ner_model: true
  skip_redundant_processing: true
  optimized_batch_size: 8
  max_concurrent_analyses: 8
  memory_optimization: true
  fast_inference: true
  chat_enabled: true
  multi_purpose_dialogpt: true
  fact_checking_enabled: true

# Multi-Model Performance Settings
multi_model_performance:
  enable_model_switching: true
  parallel_model_loading: true
  shared_model_caching: true
  intelligent_model_selection: true

# Chat Performance Settings
chat_performance:
  enable_response_caching: true
  max_concurrent_chats: 4
  response_cache_ttl: 300
  fast_response_mode: true
  fact_check_chat_responses: true

# Performance Monitoring
performance:
  track_inference_time: true
  monitor_memory_usage: true
  log_performance_metrics: true
  track_model_usage: true
  monitor_dialogpt_efficiency: true
  target_inference_time_ms: 2000
  target_chat_response_ms: 1000
  target_text_generation_ms: 500

# Resource Optimization
resources:
  max_memory_gb: 4
  cpu_threads: 8
  enable_model_caching: true
  lazy_loading: true
  chat_memory_limit_mb: 512
  dialogpt_memory_limit_mb: 1024
  meditron_memory_limit_mb: 2048
