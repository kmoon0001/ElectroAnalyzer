import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from src.guideline_service import GuidelineService

logger = logging.getLogger(__name__)

class LLMComplianceAnalyzer:
    """
    Analyzes documents for compliance using a RAG pipeline with a quantized LLM.
    """

    def __init__(self, generator_model_name: str = 'nabilfaieaz/tinyllama-med-full', guideline_service: GuidelineService = None):
        """
        Initializes the LLMComplianceAnalyzer.

        Args:
            generator_model_name (str): The name of the generator model to use from Hugging Face.
            guideline_service (GuidelineService): An instance of the GuidelineService for retrieving relevant guidelines.
        """
        logger.info(f"Initializing LLMComplianceAnalyzer with model: {generator_model_name}")

        self.guideline_service = guideline_service or GuidelineService()

        # Configure quantization to load the model in 4-bit
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(generator_model_name)
        self.generator_model = AutoModelForCausalLM.from_pretrained(
            generator_model_name,
            quantization_config=quantization_config,
            device_map="auto"
        )

        logger.info(f"Generator LLM '{generator_model_name}' loaded successfully.")

    def analyze_document(self, document_text: str) -> str:
        """
        Analyzes the document using the RAG pipeline.

        Args:
            document_text (str): The text of the document to analyze.

        Returns:
            str: The compliance analysis report generated by the LLM.
        """
        logger.info("Starting compliance analysis...")

        # 1. Retrieve relevant guidelines
        logger.info("Retrieving relevant guidelines...")
        # Use a query derived from the document's content. Using the first 512 chars as a proxy for the main topic.
        query = document_text[:512]
        retrieved_guidelines = self.guideline_service.search(query=query, top_k=3)

        context = "\n".join([f"- {g['source']}: {g['text']}" for g in retrieved_guidelines])

        # 2. Construct the prompt
        prompt = f"""
        **Instruction:** You are a medical compliance expert. Your task is to analyze the following clinical note to identify potential compliance issues based on the provided Medicare guidelines. Focus on identifying discrepancies, omissions, or statements that may not align with the best practices outlined in the guidelines. Provide a detailed analysis and quote the parts of the note that are relevant to your findings.

        **Provided Medicare Guidelines:**
        {context}

        **Clinical Note to Analyze:**
        {document_text}

        **Analysis:**
        """

        # 3. Generate the analysis
        logger.info("Generating analysis with the LLM...")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.generator_model.device)

        # Generate text with a reasonable max length
        output = self.generator_model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )

        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)

        # Extract only the analysis part from the generated text
        analysis_section = generated_text.split("**Analysis:**")[-1].strip()

        logger.info("Compliance analysis finished.")
        return analysis_section
