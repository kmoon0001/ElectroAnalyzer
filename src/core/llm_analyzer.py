import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM
from src.guideline_service import GuidelineService
from src.utils import load_config

logger = logging.getLogger(__name__)

class LLMComplianceAnalyzer:
    """
    Analyzes documents for compliance using a RAG pipeline with a quantized LLM.
    """

    def __init__(self, guideline_service: GuidelineService = None):
        """
        Initializes the LLMComplianceAnalyzer.

        Args:
            guideline_service (GuidelineService): An instance of the GuidelineService for retrieving relevant guidelines.
        """
        self.config = load_config()
        generator_model_name = self.config['models']['generator']

        logger.info(f"Initializing LLMComplianceAnalyzer with model: {generator_model_name}")

        self.guideline_service = guideline_service or GuidelineService()

        # Configure quantization to load the model in 4-bit
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(generator_model_name)
        self.generator_model = AutoModelForCausalLM.from_pretrained(
            generator_model_name,
            quantization_config=quantization_config,
            device_map="auto"
        )

        logger.info(f"Generator LLM '{generator_model_name}' loaded successfully.")

    def analyze_document(self, document_text: str) -> str:
        """
        Analyzes the document using an iterative RAG pipeline.

        Args:
            document_text (str): The text of the document to analyze.

        Returns:
            str: The compliance analysis report generated by the LLM.
        """
        logger.info("Starting compliance analysis...")
        max_iterations = self.config.get('iterative_retrieval', {}).get('max_iterations', 1)
        top_k = self.config.get('retrieval_settings', {}).get('similarity_top_k', 3)

        context = ""
        excluded_sources = []

        for i in range(max_iterations):
            current_iteration = i + 1
            logger.info(f"Iteration {current_iteration}/{max_iterations}")

            with open("src/core/prompt_template.txt", "r") as f:
                prompt_template = f.read()

            prompt = prompt_template.format(
                context=context,
                document_text=document_text,
                current_iteration=current_iteration,
                max_iterations=max_iterations
            )

            logger.info("Generating analysis with the LLM...")
            tokenized_inputs = self.tokenizer(prompt, return_tensors="pt")
            if isinstance(tokenized_inputs, str):
                tokenized_inputs = self.tokenizer.encode_plus(prompt, return_tensors="pt")
            device = next(self.generator_model.parameters()).device
            inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}

            output = self.generator_model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.95,
                do_sample=True
            )

            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            analysis_section = generated_text.split("**Analysis:**")[-1].strip()

            if "[SEARCH]" not in analysis_section:
                logger.info("Final analysis generated.")
                return analysis_section

            search_query = analysis_section.replace("[SEARCH]", "").strip()
            logger.info(f"LLM requested a search for: '{search_query}'")



            retrieved_guidelines = self.guideline_service.search(
                query=search_query,
                top_k=top_k,
                exclude_sources=list(set(excluded_sources))
            )

            if not retrieved_guidelines:
                logger.info("No new guidelines found for the search query.")
                return analysis_section # Or some other message indicating no new info

            new_context = "\n".join([f"- {g['source']}: {g['text']}" for g in retrieved_guidelines])
            context += "\n" + new_context

            for g in retrieved_guidelines:
                excluded_sources.append(g['source'])

            # Summarize context if it gets too long
            if len(self.tokenizer.encode(context)) > self.config.get('iterative_retrieval', {}).get('max_context_length', 3072):
                logger.info("Context length exceeded, summarizing...")
                context = self._summarize_context(context)

        logger.info("Max iterations reached. Returning final analysis.")
        return analysis_section

    def _summarize_context(self, context: str) -> str:
        """
        Summarizes the context if it exceeds a certain length.
        """
        summarizer_model_name = self.config['models']['summarizer']
        summarizer_tokenizer = AutoTokenizer.from_pretrained(summarizer_model_name)
        summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(summarizer_model_name)

        inputs = summarizer_tokenizer(
            "summarize: " + context,
            return_tensors="pt",
            max_length=1024,
            truncation=True
        )

        device = next(self.generator_model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        summary_ids = summarizer_model.generate(
            inputs['input_ids'],
            max_length=512,
            num_beams=4,
            early_stopping=True
        )

        summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        logger.info(f"Summarized context: {summary}")
        return summary
