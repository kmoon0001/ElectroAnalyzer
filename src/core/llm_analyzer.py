import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from src.guideline_service import GuidelineService
from src.utils import load_config

logger = logging.getLogger(__name__)

class LLMComplianceAnalyzer:
    """
    Analyzes documents for compliance using a RAG pipeline with a quantized LLM.
    """

    def __init__(self, guideline_service: GuidelineService = None):
        """
        Initializes the LLMComplianceAnalyzer.

        Args:
            guideline_service (GuidelineService): An instance of the GuidelineService for retrieving relevant guidelines.
        """
        self.config = load_config()
        generator_model_name = self.config['models']['generator']

        logger.info(f"Initializing LLMComplianceAnalyzer with model: {generator_model_name}")

        self.guideline_service = guideline_service or GuidelineService()

        # Configure quantization to load the model in 4-bit
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(generator_model_name)
        self.generator_model = AutoModelForCausalLM.from_pretrained(
            generator_model_name,
            quantization_config=quantization_config,
            device_map="auto"
        )

        logger.info(f"Generator LLM '{generator_model_name}' loaded successfully.")

    def analyze_document(self, document_text: str) -> str:
        """
        Analyzes the document using the RAG pipeline.

        Args:
            document_text (str): The text of the document to analyze.

        Returns:
            str: The compliance analysis report generated by the LLM.
        """
        logger.info("Starting compliance analysis...")

        # 1. Retrieve relevant guidelines
        logger.info("Retrieving relevant guidelines...")
        # Use a query derived from the document's content. Using the first 512 chars as a proxy for the main topic.
        query = document_text[:512]
        top_k = self.config['retrieval_settings']['similarity_top_k']
        retrieved_guidelines = self.guideline_service.search(query=query, top_k=top_k)

        context = "\n".join([f"- {g['source']}: {g['text']}" for g in retrieved_guidelines])

        # 2. Load and construct the prompt from the template file
        with open("src/core/prompt_template.txt", "r") as f:
            prompt_template = f.read()

        prompt = prompt_template.format(context=context, document_text=document_text)

        # 3. Generate the analysis
        logger.info("Generating analysis with the LLM...")
        tokenized_inputs = self.tokenizer(prompt, return_tensors="pt")
        device = next(self.generator_model.parameters()).device
        inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}

        # Generate text with a reasonable max length
        output = self.generator_model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )

        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)

        # Extract only the analysis part from the generated text
        analysis_section = generated_text.split("**Analysis:**")[-1].strip()

        logger.info("Compliance analysis finished.")
        return analysis_section
