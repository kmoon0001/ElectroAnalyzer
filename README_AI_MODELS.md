# AI Models Guide - Quick Reference

## Current Status

‚úÖ **Mock AI**: Working perfectly (fast, no downloads)
‚ö†Ô∏è **Meditron-7B**: Crashes with ctransformers
üéØ **Recommended**: Llama 3.2 3B Instruct

## Three Options

### Option 1: Keep Using Mocks (Current) ‚úÖ
**Best for**: Testing, development, low-resource systems

```yaml
use_ai_mocks: true
```

**Pros:**
- ‚úÖ No downloads
- ‚úÖ Fast (1-2 seconds)
- ‚úÖ No crashes
- ‚úÖ Works on any system

**Cons:**
- ‚ùå Generic responses
- ‚ùå No real medical analysis

---

### Option 2: Use Llama 3.2 3B (Recommended) üéØ
**Best for**: Real compliance analysis with stability

**Quick Test:**
```batch
TEST_REAL_AI.bat
```

This will:
1. Backup your current config
2. Download Llama 3.2 3B (~2GB, first time only)
3. Test if it works on your system
4. Keep or restore based on results

**Pros:**
- ‚úÖ Real AI analysis
- ‚úÖ Modern model (Sept 2024)
- ‚úÖ Stable with ctransformers
- ‚úÖ Good medical reasoning
- ‚úÖ Fast inference

**Cons:**
- ‚ö†Ô∏è 2GB download
- ‚ö†Ô∏è Needs 4-6GB RAM

---

### Option 3: Use External AI Server üîß
**Best for**: If local models keep crashing

**Install Ollama** (easiest):
```bash
# Download from: https://ollama.ai
# Then run:
ollama run llama3.2:3b
```

Then modify your app to use `http://localhost:11434` instead of local ctransformers.

---

## Model Comparison

| Model | Size | RAM | Speed | Quality | Stability | Medical |
|-------|------|-----|-------|---------|-----------|---------|
| **Mocks** | 0 | 0 | ‚ö°‚ö°‚ö° | ‚≠ê | ‚úÖ‚úÖ‚úÖ | ‚ùå |
| **Llama 3.2 3B** | 2GB | 4-6GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ‚úÖ‚úÖ | ‚≠ê‚≠ê‚≠ê |
| **Qwen2.5 3B** | 2GB | 4-6GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ‚úÖ‚úÖ | ‚≠ê‚≠ê‚≠ê |
| **Phi-3.5 Mini** | 2.3GB | 4-6GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ‚úÖ‚úÖ | ‚≠ê‚≠ê |
| **Mistral 7B** | 4GB | 6-8GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ‚úÖ | ‚≠ê‚≠ê‚≠ê |
| **Meditron 7B** | 4GB | 6-8GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## My Expert Recommendation

### For Your System:

1. **Start with Mocks** (current) ‚úÖ
   - Get the app working end-to-end
   - Test all features
   - No risk of crashes

2. **Test Llama 3.2 3B** when ready üéØ
   - Run `TEST_REAL_AI.bat`
   - If it works: Great! Real AI enabled
   - If it crashes: Stay with mocks or try Ollama

3. **Don't use Meditron** ‚ö†Ô∏è
   - Known stability issues
   - Not worth the hassle
   - Modern general models work better

---

## Quick Commands

### Test Real AI
```batch
TEST_REAL_AI.bat
```

### Launch with Mocks (Current)
```batch
LAUNCH_COMPLETE_APP.bat
```

### Switch Back to Mocks
```batch
copy config_backup.yaml config.yaml
```

### Switch to Llama 3.2 3B
```batch
copy config_llama32.yaml config.yaml
```

---

## Files Reference

- `config.yaml` - Current config (mocks enabled)
- `config_llama32.yaml` - Llama 3.2 3B config (ready to use)
- `config_tinyllama.yaml` - TinyLlama 1.1B config (smallest option)
- `BEST_LLM_OPTIONS.md` - Detailed model comparison
- `SMALLER_MODELS_GUIDE.md` - All model options explained

---

## Bottom Line

**You have a working app with mocks.** That's great for testing!

When you want real AI:
1. Run `TEST_REAL_AI.bat`
2. Wait for download (~5-10 min first time)
3. If it works: ‚úÖ You're done!
4. If it crashes: Stay with mocks or use Ollama

**Don't waste time fighting Meditron.** Llama 3.2 3B is newer, smaller, more stable, and will give you better results.
