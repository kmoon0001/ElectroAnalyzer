analysis:
  chunk_overlap: 100
  confidence_threshold: 0.75
  deterministic_focus: '- Treatment frequency documented\n- Goals reviewed or adjusted\n-
    Medical necessity justified'
  max_document_length: 50000
use_ai_mocks: false  # REAL AI ENABLED with TinyLlama
auth:
  access_token_expire_minutes: 30
  algorithm: HS256
database:
  echo: false
  url: sqlite:///./compliance.db
enable_director_dashboard: true
habits_framework:
  enabled: true
  visibility_level: moderate
llm:
  context_length: 2048  # Reduced for stability
  gpu_layers: 0
  threads: 4  # Reduced for stability
  hf_model_type: llama
  generation_params:
    max_new_tokens: 512  # Reduced for speed
    repeat_penalty: 1.1
    stop_sequences:
    - </analysis>
    - \n\n---
    temperature: 0.1
    top_p: 0.9
  model_type: ctransformers
maintenance:
  purge_interval_days: 1
  purge_retention_days: 30
models:
  analysis_prompt_template: src/resources/prompts/analysis_prompt_template.txt
  doc_classifier_prompt: src/resources/prompts/doc_classifier_prompt.txt
  fact_checker: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
  generator: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF  # CHANGED: Smaller, stable model
  generator_filename: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf  # CHANGED: ~700MB
  generator_local_path: models/tinyllama  # CHANGED: New cache location
  generator_profiles:
    clinical_fast:
      batch_size: 2
      do_sample: false
      fast_mode: true
      filename: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
      max_length: 512
      max_system_gb: 4.0  # Much lower requirement
      max_tokens: 256
      num_beams: 1
      optimize_for_speed: true
      repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
      revision: main
      temperature: 0.1
      use_cache: true
  ner_ensemble:
  - d4data/biomedical-ner-all
  - OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M
  nlg_prompt_template: src/resources/prompts/nlg_prompt_template.txt
  optimization:
    cache_models: true
    compile_models: false  # Disabled for stability
    optimize_for_inference: true
    use_better_transformer: false  # Disabled for stability
    use_flash_attention: false  # Disabled for stability
  reranker: cross-encoder/ms-marco-MiniLM-L-6-v2
  retriever: pritamdeka/S-PubMedBert-MS-MARCO
paths:
  api_url: http://127.0.0.1:8001
  cache_dir: .cache
  logs_dir: logs
  medical_dictionary: src/resources/medical_dictionary.txt
  rule_dir: ./src/resources
  temp_upload_dir: ./temp
performance:
  batch_processing: true
  batch_size: 2  # Reduced
  context_optimization: true
  enable_caching: true
  fast_mode: true  # Enabled for speed
  gpu_acceleration: false  # Disabled for stability
  gradient_checkpointing: false  # Disabled
  inference_optimization: true
  max_chunk_size: 1500  # Reduced
  max_workers: 2
  memory_efficient_mode: true
  memory_optimization: true
  mixed_precision: false  # Disabled for stability
  model_quantization: true
  optimized_chunking: true
  overlap_size: 150  # Reduced
  skip_advanced_ner: false
  skip_fact_checking: false
  enable_deep_fact_checking: false  # Disabled for speed
  rag_fact_checking:
    similarity_threshold: 0.7
    rerank_threshold: 0.8
  parallel_processing: true
  reduce_context_window: true  # Enabled for smaller model
  simple_report_mode: false
  smart_caching: true
retrieval:
  batch_size: 16
  dense_model_name: pritamdeka/S-PubMedBert-MS-MARCO
  max_sequence_length: 512
  rrf_k: 60
  similarity_top_k: 10
log_level: INFO
