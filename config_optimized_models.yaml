# Optimized Clinical Document Analyzer Configuration
# Uses quantized and distilled models for better accuracy and stability

# Application Settings
app_name: "Therapy Compliance Analyzer"
version: "2.0.0"
debug: false

# AI Configuration
use_ai_mocks: false  # REAL AI with optimized models

# Database Configuration
database:
  url: "sqlite:///./compliance.db"
  echo: false

# LLM Configuration - Meditron 7B Q5_K_M (Higher Quality)
llm:
  context_length: 4096
  gpu_layers: 0
  threads: 6
  hf_model_type: meditron
  model_type: ctransformers
  model_repo_id: TheBloke/meditron-7B-GGUF
  local_model_path: "models/meditron7b-q5/meditron-7b.Q5_K_M.gguf"
  generation_params:
    max_new_tokens: 1024
    repeat_penalty: 1.1
    stop_sequences:
    - </analysis>
    - "\n\n---"
    temperature: 0.1
    top_p: 0.9

# Generator Profiles - Optimized Models
generator_profiles:
  clinical_optimized:
    name: "Meditron 7B Q5_K_M - Optimized Medical Analysis"
    description: "Higher quality quantization for better medical compliance analysis"
    generator: "meditron-7b-q5"
    local_path: "models/meditron7b-q5"
    repo: "TheBloke/meditron-7B-GGUF"
    model_file: "meditron-7b.Q5_K_M.gguf"
    context_length: 4096
    max_tokens: 1024
    temperature: 0.1
    quantization: "Q5_K_M"
    accuracy_improvement: "5% better than Q4_K_M"

  clinical_backup:
    name: "DialoGPT Medium - Backup Analysis"
    description: "Backup conversational analysis"
    generator: "dialogpt-medium"
    local_path: "models/dialogpt-medium"
    repo: "microsoft/DialoGPT-medium"
    context_length: 1024
    max_tokens: 512
    temperature: 0.7

# NER Models - Optimized Versions
ner_models:
  biomedical_distilled:
    name: "Distilled Biomedical NER - Primary"
    model_id: "d4data/biomedical-ner-all"
    local_path: "models/biomedical-ner-distilled"
    priority: 1
    type: "distilled"
    size_reduction: "60%"

  pathology_quantized:
    name: "Quantized Pathology Detection - Secondary"
    model_id: "OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M"
    local_path: "models/openmed-ner-quantized"
    priority: 2
    type: "quantized"
    size_reduction: "62%"

# Embedding Models - Distilled Version
embedding_models:
  distilbert:
    name: "DistilBERT - Optimized Embeddings"
    model_id: "sentence-transformers/distilbert-base-nli-mean-tokens"
    local_path: "models/distilbert-embeddings"
    dimension: 768
    type: "distilled"
    size_reduction: "50%"

# Analysis Settings
analysis:
  max_file_size_mb: 50
  supported_formats: ["pdf", "docx", "txt", "html"]
  phi_redaction: true
  confidence_threshold: 0.7

# API Settings
api:
  host: "127.0.0.1"
  port: 8001
  cors_origins: ["http://127.0.0.1:3001", "http://localhost:3001"]

# Logging
logging:
  level: "INFO"
  format: "json"
  file: "logs/app.log"

# Cache Settings
cache:
  enabled: true
  ttl_seconds: 3600
  max_size_mb: 1000

# Model Optimization Settings
optimization:
  enable_model_caching: true
  parallel_ner_processing: true
  batch_size: 4  # Increased due to smaller models
  max_concurrent_analyses: 5  # Increased due to efficiency
  memory_optimization: true
  quantization_benefits:
    - "Better accuracy than Q4_K_M"
    - "Faster inference"
    - "Lower memory usage"
    - "More stable performance"

# Performance Monitoring
performance:
  track_model_accuracy: true
  monitor_inference_time: true
  log_quantization_metrics: true
