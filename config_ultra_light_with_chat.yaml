# Ultra-Light Clinical Document Analyzer with Chat
# Maximum performance with minimal resource usage + lightweight chat

# Application Settings
app_name: "Therapy Compliance Analyzer"
version: "2.0.0"
debug: false

# AI Configuration
use_ai_mocks: false  # REAL AI with ultra-light models

# Database Configuration
database:
  url: "sqlite:///./compliance.db"
  echo: false

# LLM Configuration - Meditron 7B Q4_K_S (Ultra-Light)
llm:
  context_length: 4096
  gpu_layers: 0
  threads: 8
  hf_model_type: meditron
  model_type: ctransformers
  model_repo_id: TheBloke/meditron-7B-GGUF
  local_model_path: "models/meditron7b-ultra/meditron-7b.Q4_K_S.gguf"
  generation_params:
    max_new_tokens: 1024
    repeat_penalty: 1.1
    stop_sequences:
    - </analysis>
    - "\n\n---"
    temperature: 0.1
    top_p: 0.9

# Chat Configuration - Ultra-Light
chat:
  enabled: true
  model: "dialogpt-medium"  # Lightweight chat model
  local_path: "models/dialogpt-medium"
  max_tokens: 256  # Shorter responses for speed
  temperature: 0.7
  context_length: 512  # Smaller context for performance
  response_timeout: 5  # 5 second timeout

# Generator Profiles - Ultra-Light Models
generator_profiles:
  clinical_ultra_light:
    name: "Meditron 7B Q4_K_S - Ultra-Light Medical Analysis"
    description: "Ultra-light quantization for maximum performance"
    generator: "meditron-7b-ultra"
    local_path: "models/meditron7b-ultra"
    repo: "TheBloke/meditron-7B-GGUF"
    model_file: "meditron-7b.Q4_K_S.gguf"
    context_length: 4096
    max_tokens: 1024
    temperature: 0.1
    quantization: "Q4_K_S"
    performance_improvement: "25% smaller, 2x faster"

  chat_light:
    name: "DialoGPT Medium - Lightweight Chat"
    description: "Fast conversational AI for chat assistant"
    generator: "dialogpt-medium"
    local_path: "models/dialogpt-medium"
    repo: "microsoft/DialoGPT-medium"
    context_length: 512
    max_tokens: 256
    temperature: 0.7
    type: "chat"
    performance_optimized: true

# NER Models - Single Ultra-Light Model Only
ner_models:
  biomedical_ultra:
    name: "Ultra-Distilled Biomedical NER"
    model_id: "d4data/biomedical-ner-all"
    local_path: "models/biomedical-ner-ultra"
    priority: 1
    type: "ultra-distilled"
    size_reduction: "70%"

# Embedding Models - Ultra-Light Version
embedding_models:
  distilbert_ultra:
    name: "DistilBERT Ultra-Light Embeddings"
    model_id: "sentence-transformers/distilbert-base-nli-mean-tokens"
    local_path: "models/distilbert-ultra"
    dimension: 768
    type: "ultra-distilled"
    size_reduction: "60%"

# Analysis Settings - Optimized for Performance
analysis:
  max_file_size_mb: 50
  supported_formats: ["pdf", "docx", "txt", "html"]
  phi_redaction: true
  confidence_threshold: 0.7

# API Settings
api:
  host: "127.0.0.1"
  port: 8001
  cors_origins: ["http://127.0.0.1:3001", "http://localhost:3001"]

# Logging
logging:
  level: "INFO"
  format: "json"
  file: "logs/app.log"

# Cache Settings - Optimized for Performance
cache:
  enabled: true
  ttl_seconds: 7200
  max_size_mb: 500

# Ultra-Light Performance Settings
ultra_light_mode:
  enabled: true
  single_ner_model: true
  skip_redundant_processing: true
  optimized_batch_size: 8
  max_concurrent_analyses: 8
  memory_optimization: true
  fast_inference: true
  chat_enabled: true
  chat_performance_mode: true  # Optimize chat for speed

# Chat Performance Settings
chat_performance:
  enable_response_caching: true
  max_concurrent_chats: 4
  response_cache_ttl: 300  # 5 minutes
  fast_response_mode: true
  skip_complex_reasoning: false  # Keep medical accuracy

# Performance Monitoring
performance:
  track_inference_time: true
  monitor_memory_usage: true
  log_performance_metrics: true
  target_inference_time_ms: 2000
  target_chat_response_ms: 1000  # 1 second chat responses

# Resource Optimization
resources:
  max_memory_gb: 4
  cpu_threads: 8
  enable_model_caching: true
  lazy_loading: true
  chat_memory_limit_mb: 512  # Limit chat memory usage
