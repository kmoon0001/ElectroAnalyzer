# config.yaml
APP_NAME: Therapy Compliance Analyzer
ENVIRONMENT: production
DB_PATH: ./data/compliance.db
ENCRYPTION_KEY: "" # Replace with your generated key

# OCR settings
ocr:
  preprocessing: true
  deskew: true
  resolution: 300

# Configuration for AI Models and Services (Optimized for Performance)
models:
  # NER Ensemble: A list of models for comprehensive entity recognition
  ner_ensemble:
    - "dslim/bert-base-NER"                       # A reliable, general-purpose NER model
    - "elastic/distilbert-base-cased-finetuned-conll03-english" # A fast and reliable generalist
  
  # Best-in-class sentence transformer for medical semantic search
  retriever: "pritamdeka/S-PubMedBert-MS-MARCO"
  
  # Quantized, locally-deployable medical text generator
  generator: "andrijdavid/TinyLlama-1.1B-Chat-v1.0-GGUF"
  generator_filename: "TinyLlama-1.1B-Chat-v1.0-Q4_K_M.gguf" # A popular quantized version

  # Small, fast model for fact-checking and validation
  fact_checker: "google/flan-t5-small"

  # Prompt Templates
  analysis_prompt_template: "src/core/prompt_template.txt"
  nlg_prompt_template: "src/core/nlg_prompt_template.txt"
  doc_classifier_prompt: "src/core/doc_classifier_prompt.txt"

# Settings for the generator model
llm_settings:
  gpu_layers: 0  # Offload all possible layers to GPU for GGUF models
  model_type: "llama"
  context_length: 2048
  generation_params:
    max_new_tokens: 1024
    temperature: 0.3
    top_p: 0.95
    repetition_penalty: 1.1

# Retrieval settings
retrieval_settings:
  similarity_top_k: 3

# Database Maintenance
purge_retention_days: 60
